{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8becaf23-dade-4b5b-b1f8-1bcd2e206ce1",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "\n",
    "Based in the proposed paper : https://arxiv.org/abs/1511.06434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524e88c2-e6a9-4782-b119-3e1b704f424e",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61135ded-a062-4c81-b70c-d3196aa2c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-msssim\n",
      "  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (from pytorch-msssim) (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch->pytorch-msssim) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch->pytorch-msssim) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->pytorch-msssim) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->pytorch-msssim) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch->pytorch-msssim) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch->pytorch-msssim) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch->pytorch-msssim) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->pytorch-msssim) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->pytorch-msssim) (2.1.3)\n",
      "Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n",
      "Installing collected packages: pytorch-msssim\n",
      "Successfully installed pytorch-msssim-1.0.0\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-msssim\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b75f1c9-9904-4639-be12-daa8f2b3fc50",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ed48c60-392a-4d72-bd36-307958508296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from pytorch_msssim import ssim  # For SSIM metric\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6343e71-0c0c-4e11-9e97-3924dbfa1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device as cuda if available\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8141784f-1f4d-4168-be20-c2514d26d693",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16fc01af-5e69-4b18-a858-b269650087ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 26.4M/26.4M [00:15<00:00, 1.74MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 29.5k/29.5k [00:00<00:00, 930kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4.42M/4.42M [00:02<00:00, 1.70MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5.15k/5.15k [00:00<00:00, 6.70MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86154ed1-4b78-4f91-8e70-43f1379daf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of channels\n",
    "nc=1\n",
    "\n",
    "# input noise dimension\n",
    "nz = 100\n",
    "\n",
    "# number of generator filters\n",
    "ngf = 64\n",
    "\n",
    "#number of discriminator filters\n",
    "ndf = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786f899-9009-4817-9286-298e303ca790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline from LAB 5(INM705 Deep Learning for Image Analysis)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc, ndf):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.nc = nc\n",
    "        self.ndf = ndf\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(self.nc, self.ndf, 5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(self.ndf,self.ndf*2, 5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((self.ndf*2)*7*7, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2615f-cd0d-4653-93d3-17487df3c5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline from LAB 5 (INM705 Deep Learning for Image Analysis)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "      def __init__(self, nc, ngf, nz):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nc = nc\n",
    "        self.ngf = ngf\n",
    "        self.nz = nz\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 7*7*(self.ngf*4), bias=False),\n",
    "            nn.BatchNorm1d(7*7*(self.ngf*4)),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.Unflatten(self.nc, (self.ngf*4, 7, 7)),\n",
    "            nn.ConvTranspose2d(self.ngf*4, self.ngf*2, 5, stride=1, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf*2),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.ConvTranspose2d(self.ngf*2, self.ngf, 5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.LeakyReLU(0.3),\n",
    "            nn.ConvTranspose2d(self.ngf, self.nc, 5, stride=2, padding=2, output_padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "      def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403ee58-f114-4274-9a4e-160f9be2de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and assign models to the device\n",
    "\n",
    "D = Discriminator(nc,ndf).to(device)\n",
    "G = Generator(nc,ngf,nz).to(device)\n",
    "\n",
    "print(D)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31f66d7-4d16-43d1-897f-4168c411c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizers\n",
    "\n",
    "optimizerD = optim.Adam(D.parameters(), lr=1e-4)\n",
    "optimizerG = optim.Adam(G.parameters(), lr=1e-4)\n",
    "\n",
    "# Define loss\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f307fed0-5245-4f86-b395-e23d8b6ad69d",
   "metadata": {},
   "source": [
    "## Function to save images generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b314fb6-6efb-426a-8460-5de2bf7c09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(epoch):\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    fixed_noise = torch.randn(32, 100, device=device)\n",
    "    fake_images = G(fixed_noise).detach().cpu()\n",
    "    save_image(fake_images, f'outputs/epoch_{epoch}.png', nrow=8, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a13c4ff-343a-4109-b777-9dd063d195b8",
   "metadata": {},
   "source": [
    "## Metrics definition\n",
    "\n",
    "The following metrics are based on the following source: https://medium.com@heyamit10/pytorch-implementation-of-common-gan-metrics-86f993f6e737\n",
    "\n",
    "Changes were made to the implementation of the methods in order to modify them for the dataset used in this work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2aa5ac-ba8f-41dd-8ad1-7ca714d961c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KID (Kernel Inception Distance)\n",
    "\n",
    "def polynomial_mmd(x, y, degree=3, gamma=None, coef0=1):\n",
    "    x = x.reshape(x.size(0), -1)  # Reshape to 2D\n",
    "    y = y.reshape(y.size(0), -1)  # Reshape to 2D\n",
    "    if gamma is None:\n",
    "        gamma = 1.0 / x.shape[1]\n",
    "    kernel_xx = (gamma * x.mm(x.t()) + coef0) ** degree\n",
    "    kernel_yy = (gamma * y.mm(y.t()) + coef0) ** degree\n",
    "    kernel_xy = (gamma * x.mm(y.t()) + coef0) ** degree\n",
    "    return kernel_xx.mean() + kernel_yy.mean() - 2 * kernel_xy.mean()\n",
    "\n",
    "\n",
    "\n",
    "def kernel_inception_distance(real_features, generated_features):\n",
    "    real_features, generated_features = torch.tensor(real_features), torch.tensor(generated_features)\n",
    "    return polynomial_mmd(real_features, generated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d1a3b-a3ce-408b-aaee-c368b2fcd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Inception model\n",
    "\n",
    "inception = models.inception_v3(pretrained=True).eval()\n",
    "inception.fc = torch.nn.Identity()  # Remove last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e4f2e7-5129-4bd2-bc8a-ab961b168859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision and recall\n",
    "\n",
    "def get_features(images, model, batch_size=32):\n",
    "    features = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = images[i:i + batch_size].cuda()\n",
    "        with torch.no_grad():\n",
    "            batch_features = model(batch).cpu().numpy()\n",
    "        features.append(batch_features)\n",
    "    return np.concatenate(features)\n",
    "\n",
    "def compute_precision_recall(real_features, generated_features, k=5):\n",
    "    real_features = real_features.reshape(real_features.size(0), -1).cpu().numpy()  # Reshape to 2D and convert to numpy\n",
    "    generated_features = generated_features.reshape(generated_features.size(0), -1).cpu().numpy()  # Reshape to 2D and convert to numpy\n",
    "    # Initialize FAISS index for nearest neighbors\n",
    "    index = faiss.IndexFlatL2(real_features.shape[1])\n",
    "    index.add(real_features)\n",
    "\n",
    "    # Precision: Nearest neighbors for generated samples in real set\n",
    "    D, I = index.search(generated_features, k)\n",
    "    precision = np.mean([np.any(i in I for i in range(len(real_features))) for _ in D])\n",
    "\n",
    "    # Recall: Nearest neighbors for real samples in generated set\n",
    "    index.reset()\n",
    "    index.add(generated_features)\n",
    "    D, I = index.search(real_features, k)\n",
    "    recall = np.mean([np.any(i in I for i in range(len(generated_features))) for _ in D])\n",
    "\n",
    "    return precision, recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
